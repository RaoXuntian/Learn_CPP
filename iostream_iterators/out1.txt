!= ( 1 ) 
" ( 1 ) 
"+" ( 1 ) 
"/" ( 1 ) 
"/part-r-00000")); ( 2 ) 
":" ( 2 ) 
"ClassWordCount"); ( 1 ) 
"Classify"); ( 1 ) 
"ExistingWord"); ( 1 ) 
"MultiFileWordCount"); ( 1 ) 
"\t" ( 4 ) 
"utf-8"); ( 2 ) 
(ClassTotalWords.get(newKey.toString())+ ( 1 ) 
(FileSplit)inputSplit; ( 1 ) 
(IntWritable)ReflectionUtils.newInstance(reader.getValueClass(), ( 1 ) 
(IntWritable)ReflectionUtils.newInstance(reader1.getValueClass(), ( 1 ) 
(IntWritable)ReflectionUtils.newInstance(reader2.getValueClass(), ( 1 ) 
(IntWritable)ReflectionUtils.newInstance(reader3.getValueClass(), ( 1 ) 
(Text) ( 1 ) 
(Text)ReflectionUtils.newInstance(reader1.getKeyClass(), ( 1 ) 
(Text)ReflectionUtils.newInstance(reader2.getKeyClass(), ( 1 ) 
(Text)ReflectionUtils.newInstance(reader3.getKeyClass(), ( 1 ) 
(fn) ( 1 ) 
(fp) ( 1 ) 
(itr.hasMoreTokens()) ( 1 ) 
(tn) ( 1 ) 
(tp) ( 1 ) 
(value3.get()+1) ( 1 ) 
(value3.get()+1)/(ClassTotalWords.get(newKey.toString())+TotalDiffWords)); ( 1 ) 
); ( 1 ) 
+ ( 19 ) 
+= ( 4 ) 
+TotalDiffWords)); ( 1 ) 
+newValue); ( 1 ) 
-Averaging）：是先对每一个类统计指标值，然后在对所有类求算术平均值。 ( 1 ) 
-put命令将数据上传至HDFS，数据上传过程花了将近40分钟，主要是因为小文件过多，还有一个原因是副本数配置为2，所以除了上传原文件之外，每个文件还会额外创建一个副本。以后可以将小文件合并为Sequence文件再上传至HDFS处理，增加处理速度。 ( 1 ) 
... ( 2 ) 
...> ( 2 ) 
// ( 47 ) 
//<<class:word>,wordcounts/(classTotalNums+TotalDiffWords)> ( 1 ) 
//Key为文件路径，value为文件内容，都为Text格式 ( 1 ) 
//System.out.println(key+":"+value.get()+"/"+totalWords+"\t"+value.get()/totalWords); ( 1 ) 
//System.out.println(key1.toString() ( 1 ) 
//System.out.println(key3.toString() ( 1 ) 
//System.out.println(newKey ( 1 ) 
//job2的输入输出文件路径 ( 1 ) 
//job3的输入输出文件路径 ( 1 ) 
//job的输入输出文件路径 ( 1 ) 
//不切分文件，一个split读入整个文件 ( 1 ) 
//主的控制容器，控制上面的子作业 ( 1 ) 
//作业之间依赖关系 ( 1 ) 
//先在tempvalue中放入类Ci的先验概率***** ( 1 ) 
//初始化概率值，并存入Hashtable ( 1 ) 
//加入控制容器 ( 4 ) 
//启动线程 ( 1 ) 
//如果作业成功完成，就打印成功作业的信息 ( 1 ) 
//如果测试文档的单词在训练集中出现过，则直接加上之前计算的概率 ( 1 ) 
//对于同一个类别没有出现过的单词的概率一样，1/(ClassTotalWords.get(class) ( 1 ) 
//将key的格式设置为'class:word' ( 1 ) 
//将文件路径作为Key,文本内容作为value ( 2 ) 
//将输出设置为Sequence类型，方便下一个Job读取处理 ( 1 ) 
//将键值设为word ( 1 ) 
//当命令行输入Hadoop作业的运行参数时，则此命令可取出剩余命令行 ( 1 ) 
//根据官方例程中的WholeFileInputFormat改写 ( 1 ) 
//检查输出目录是否存在，若存在，则删除 ( 2 ) 
//此处的Key为路径，Value为单个文件的内容 ( 1 ) 
//添加到总的JobControl里，进行控制 ( 1 ) 
//等同于 ( 2 ) 
//自定义RecordReader，读取整个小文件内容 ( 1 ) 
//计算先验概率 ( 1 ) 
//返回每个子目录下的所有文本文件 ( 2 ) 
//遍历类，每个类别中再加一个没有出现单词的概率，其格式为<class,probably> ( 1 ) 
//重写listStatus，返回所有文件列表 ( 2 ) 
0 ( 1 ) 
0, ( 4 ) 
0.0; ( 2 ) 
0; ( 2 ) 
0<文档总数<=100，测试文档数设为10 ( 1 ) 
1 ( 4 ) 
1. ( 2 ) 
1.0/(ClassTotalWords.get(entry.getKey()) ( 1 ) 
1.0f:1.0f; ( 1 ) 
1.1 ( 2 ) 
1.2 ( 2 ) 
1.3 ( 2 ) 
1.4 ( 2 ) 
1.5 ( 2 ) 
1.5.1 ( 2 ) 
1.5.2 ( 2 ) 
1.5.3 ( 2 ) 
10 ( 1 ) 
100<文档总数<=200，测试文档数设为15 ( 1 ) 
108 ( 1 ) 
11 ( 3 ) 
128G ( 3 ) 
13 ( 2 ) 
133 ( 1 ) 
139 ( 1 ) 
14 ( 2 ) 
143 ( 1 ) 
148 ( 1 ) 
15 ( 4 ) 
154 ( 1 ) 
16 ( 1 ) 
163 ( 1 ) 
17 ( 1 ) 
184 ( 1 ) 
185 ( 1 ) 
1; ( 1 ) 
2 ( 1 ) 
2. ( 2 ) 
2.00GHz ( 2 ) 
2.1 ( 2 ) 
2.1.1 ( 2 ) 
2.1.2 ( 2 ) 
2.1.3 ( 2 ) 
2.1GHz ( 1 ) 
2.2 ( 2 ) 
20 ( 4 ) 
200 ( 1 ) 
200<文档总数<=400，测试文档数设为20 ( 1 ) 
2020年 ( 1 ) 
210 ( 1 ) 
235 ( 1 ) 
237 ( 1 ) 
243 ( 1 ) 
255 ( 1 ) 
257 ( 1 ) 
263 ( 1 ) 
281 ( 1 ) 
285 ( 1 ) 
3 ( 3 ) 
3. ( 2 ) 
3.1 ( 2 ) 
3.2 ( 2 ) 
30 ( 2 ) 
305 ( 1 ) 
3107 ( 1 ) 
3137 ( 1 ) 
4 ( 2 ) 
4. ( 1 ) 
4.1 ( 2 ) 
4.2 ( 2 ) 
4.3 ( 2 ) 
400<文档总数，测试文档数设为30 ( 1 ) 
440 ( 1 ) 
45 ( 1 ) 
470 ( 1 ) 
5168 ( 1 ) 
5378 ( 1 ) 
6 ( 3 ) 
6) ( 1 ) 
6.1 ( 1 ) 
6.2 ( 1 ) 
6.3 ( 1 ) 
6.4 ( 1 ) 
6.5 ( 1 ) 
6.6 ( 1 ) 
64 ( 3 ) 
68 ( 1 ) 
6; ( 1 ) 
6核12线程 ( 1 ) 
7 ( 1 ) 
7.4 ( 3 ) 
71 ( 1 ) 
72 ( 1 ) 
8 ( 2 ) 
81 ( 1 ) 
8核16线程 ( 2 ) 
9 ( 1 ) 
93 ( 1 ) 
: ( 4 ) 
<InputDataPath> ( 1 ) 
<Output1> ( 1 ) 
<Output2> ( 1 ) 
<Output3> ( 1 ) 
<Output4>" ( 1 ) 
<TestDataPath> ( 1 ) 
= ( 116 ) 
> ( 1 ) 
? ( 1 ) 
@Override ( 11 ) 
ALB ( 1 ) 
ARG ( 1 ) 
AUSTR ( 1 ) 
ArrayList<>(); ( 2 ) 
BELG ( 1 ) 
BRAZ ( 1 ) 
CANA ( 1 ) 
CHINA ( 1 ) 
CPU型号 ( 1 ) 
CPU属性 ( 1 ) 
Cent-OS ( 3 ) 
ClassTotalWords ( 1 ) 
ClassTotalWords.put(key1.toString(), ( 1 ) 
ClassTotalWordsPath ( 1 ) 
ClassWordCount ( 5 ) 
ClassWordCountMap ( 1 ) 
ClassWordCountsPath ( 1 ) 
ClassWordCount的数据流 ( 1 ) 
Classify ( 3 ) 
ClassifyMap ( 1 ) 
ClassifyReduce ( 1 ) 
Classify数据流 ( 1 ) 
Configuration ( 5 ) 
Configuration(); ( 4 ) 
Configured ( 2 ) 
Context ( 7 ) 
ControlledJob ( 4 ) 
ControlledJob(conf); ( 4 ) 
DataPreparation ( 3 ) 
DataPreparation(), ( 1 ) 
DiffTotalWordsPath ( 1 ) 
Double.parseDouble(value.toString().substring(index+1)); ( 2 ) 
Double> ( 4 ) 
Double>(); ( 2 ) 
Double>();//每个类及类对应的单词总数 ( 1 ) 
E5-2620 ( 1 ) 
E5-2650 ( 2 ) 
Evaluation ( 1 ) 
Exception ( 4 ) 
ExistingWord ( 5 ) 
ExistingWordMap ( 1 ) 
ExistingWordReduce ( 1 ) 
ExistingWord的数据流 ( 1 ) 
F1: ( 1 ) 
FSDataInputStream ( 2 ) 
FSDataInputStream(fs.open(fileSplit.getPath())); ( 2 ) 
FileInputFormat.addInputPath(job, ( 1 ) 
FileInputFormat.addInputPath(job2, ( 1 ) 
FileInputFormat.addInputPath(job3, ( 1 ) 
FileInputFormat.addInputPaths(job1, ( 1 ) 
FileInputFormat<LongWritable, ( 1 ) 
FileOutputFormat.setOutputPath(job, ( 1 ) 
FileOutputFormat.setOutputPath(job1, ( 1 ) 
FileOutputFormat.setOutputPath(job2, ( 1 ) 
FileOutputFormat.setOutputPath(job3, ( 1 ) 
FileSplit ( 1 ) 
FileStatus[] ( 2 ) 
FileSystem ( 8 ) 
FileSystem.get(URI.create(ClassTotalWordsPath), ( 1 ) 
FileSystem.get(URI.create(ClassWordCountsPath), ( 1 ) 
FileSystem.get(URI.create(DiffTotalWordsPath), ( 1 ) 
FileSystem.get(URI.create(filePath), ( 1 ) 
FileSystem.get(conf); ( 2 ) 
GFR ( 1 ) 
GenericOptionsParser(conf, ( 2 ) 
GetConditionProbably() ( 1 ) 
GetPriorProbably() ( 1 ) 
HashMap<String, ( 2 ) 
HashTable<String,Double> ( 2 ) 
Hashtable<String, ( 4 ) 
Hashtable<String,Double> ( 2 ) 
IOException ( 3 ) 
IOException, ( 7 ) 
IOException{ ( 4 ) 
IOUtils.closeStream(in); ( 2 ) 
IOUtils.closeStream(reader); ( 1 ) 
IOUtils.closeStream(reader1); ( 1 ) 
IOUtils.closeStream(reader2); ( 1 ) 
IOUtils.closeStream(reader3); ( 1 ) 
IOUtils.readFully(in, ( 2 ) 
IntWritable ( 8 ) 
IntWritable(1); ( 2 ) 
IntWritable, ( 3 ) 
IntWritable> ( 1 ) 
IntWritable>{ ( 3 ) 
InterruptedException ( 3 ) 
InterruptedException{ ( 4 ) 
Iterable<IntWritable> ( 1 ) 
Iterable<Text> ( 1 ) 
JAP ( 1 ) 
Job ( 4 ) 
Job.getInstance(conf, ( 4 ) 
JobControl ( 1 ) 
JobControl("NaiveBayes"); ( 1 ) 
Job名 ( 1 ) 
Laplace平滑 ( 2 ) 
List<FileStatus> ( 6 ) 
M201977221 ( 1 ) 
Mapper<Text, ( 4 ) 
MapsAndReduces ( 3 ) 
Map与Reduce速度不匹配 ( 1 ) 
Map程序得到上述格式的数据后，可以根据文件路径得到类名（父目录名）与文件名（当前目录名）。并将key设置为<类名：单词>的格式，value即设置为1。 ( 1 ) 
Master ( 1 ) 
Math.log(Model.wordsProbably.get(className)); ( 1 ) 
Math.log(Model.wordsProbably.get(tempkey)); ( 1 ) 
Math.log(entry.getValue());//构建临时键值对的value为各概率相乘,转化为各概率取对数再相加 ( 1 ) 
Model ( 3 ) 
Model.GetConditionProbably(); ( 2 ) 
Model.GetPriorProbably(); ( 2 ) 
MultiFileWordCount ( 5 ) 
MultiFileWordCountMap ( 1 ) 
MultiFileWordCount数据流 ( 1 ) 
MyFileInputFormat ( 3 ) 
MyFileRecordReader ( 4 ) 
MyFileRecordReader(); ( 1 ) 
No ( 2 ) 
N是总的样本个数，k是总的类别个数，是类别为的样本个数，是平滑值。 ( 1 ) 
OutPath ( 1 ) 
OutPath); ( 1 ) 
OutPath1 ( 1 ) 
OutPath1); ( 1 ) 
OutPath2 ( 1 ) 
OutPath2); ( 1 ) 
OutPath3 ( 1 ) 
OutPath3); ( 1 ) 
OutPath4 ( 1 ) 
P(Precision,精度): ( 1 ) 
Path ( 10 ) 
Path(ClassTotalWordsPath); ( 1 ) 
Path(ClassWordCountsPath); ( 1 ) 
Path(DiffTotalWordsPath); ( 1 ) 
Path(filePath); ( 1 ) 
Path(key.toString()).getName(); ( 2 ) 
Path(key.toString()).getParent().getName() ( 1 ) 
Path(key.toString()).getParent().getName()+":"+tmp); ( 1 ) 
Path(otherArgs[1] ( 2 ) 
Path(otherArgs[1]); ( 1 ) 
Path(otherArgs[2]); ( 1 ) 
Path(otherArgs[3]); ( 1 ) 
Path(otherArgs[4])); ( 1 ) 
Path(otherArgs[5]); ( 2 ) 
Prediction ( 3 ) 
Prediction(), ( 1 ) 
Prediction.getArgs(); ( 1 ) 
P和R的调和平均： ( 1 ) 
R(Recall,精度): ( 1 ) 
RUSS ( 1 ) 
RecordReader ( 1 ) 
RecordReader<LongWritable, ( 1 ) 
RecordReader<Text, ( 2 ) 
Reducer<Text, ( 2 ) 
Reduce则直接使用自带的IntSumReducer计算总和即可。 ( 1 ) 
Reduce端将概率最大的类设置为该样本所属的类。 ( 1 ) 
ReflectionUtils.newInstance(reader.getKeyClass(), ( 1 ) 
SequenceFile.Reader ( 5 ) 
SequenceFile.Reader(conf, ( 1 ) 
SequenceFile.Reader(fs, ( 2 ) 
SequenceFile.Reader(fs1, ( 1 ) 
SequenceFile.Reader(fs2, ( 1 ) 
SequenceFile.Reader(fs3, ( 1 ) 
SequenceFile.Reader.file(path1)); ( 1 ) 
Slave1 ( 1 ) 
Slave2 ( 1 ) 
String ( 13 ) 
String(contextByte, ( 2 ) 
StringTokenizer ( 2 ) 
StringTokenizer(line); ( 1 ) 
StringTokenizer(value.toString()); ( 1 ) 
String[] ( 5 ) 
System.exit(ret); ( 2 ) 
System.out.println("Usage ( 1 ) 
System.out.println(TotalDiffWords); ( 1 ) 
System.out.println(jobCtrl.getSuccessfulJobList()); ( 1 ) 
System.out.println(key ( 1 ) 
TaskAttemptContext ( 2 ) 
Text ( 17 ) 
Text(); ( 9 ) 
Text, ( 9 ) 
Text> ( 4 ) 
Text>{ ( 2 ) 
Thread ( 1 ) 
Thread(jobCtrl); ( 1 ) 
Tool ( 2 ) 
ToolRunner.run(new ( 2 ) 
TotalDiffWords ( 2 ) 
TotalDiffWords) ( 1 ) 
TotalDiffWords)); ( 1 ) 
USA ( 1 ) 
V0 ( 2 ) 
V2 ( 1 ) 
Yes ( 2 ) 
\\将正确结果与分类结果同时打印，方便直观检查 ( 1 ) 
\t" ( 1 ) 
args) ( 4 ) 
args).getRemainingArgs(); ( 2 ) 
args); ( 2 ) 
boolean ( 5 ) 
byte[(int)fileSplit.getLength()]; ( 2 ) 
byte[] ( 2 ) 
class ( 13 ) 
className ( 2 ) 
classProbably ( 2 ) 
classProbably.put(key.toString(), ( 1 ) 
classProbably; ( 1 ) 
close(){ ( 1 ) 
conf ( 4 ) 
conf); ( 17 ) 
conf; ( 1 ) 
context ( 2 ) 
context) ( 5 ) 
context)throws ( 2 ) 
context, ( 1 ) 
context.write(key, ( 2 ) 
context.write(newKey, ( 3 ) 
context.write(word, ( 1 ) 
contextByte ( 2 ) 
contextByte, ( 2 ) 
contextByte.length); ( 4 ) 
createRecordReader(InputSplit ( 1 ) 
ctrljob1 ( 1 ) 
ctrljob1.setJob(job1); ( 1 ) 
ctrljob2 ( 1 ) 
ctrljob2.addDependingJob(ctrljob1); ( 1 ) 
ctrljob2.setJob(job2); ( 1 ) 
ctrljob3 ( 1 ) 
ctrljob3.addDependingJob(ctrljob1); ( 1 ) 
ctrljob3.setJob(job3); ( 1 ) 
ctrljob4 ( 1 ) 
ctrljob4.setJob(job); ( 1 ) 
dir ( 2 ) 
dir.getPath().getFileSystem(job.getConfiguration()).listStatus(dir.getPath()); ( 2 ) 
dirs ( 2 ) 
dirs){ ( 2 ) 
docID ( 2 ) 
double ( 4 ) 
else{//如果测试文档中出现了新单词则加上之前计算新单词概率 ( 1 ) 
entry.getKey();//类名 ( 1 ) 
entry:ClassTotalWords.entrySet()){ ( 1 ) 
entry:Model.classProbably.entrySet()){//外层循环遍历所有类别 ( 1 ) 
extends ( 11 ) 
false ( 2 ) 
false; ( 4 ) 
false;//标记,若第一次循环则先赋值,否则比较若概率更大则更新 ( 1 ) 
filePath ( 1 ) 
fileSplit.getPath().getFileSystem(conf); ( 2 ) 
fileSplit; ( 1 ) 
filename) ( 1 ) 
files ( 2 ) 
final ( 2 ) 
flag ( 2 ) 
float ( 1 ) 
for(FileStatus ( 2 ) 
for(Map.Entry<String, ( 1 ) 
for(Map.Entry<String,Double> ( 1 ) 
for(Text ( 1 ) 
fs ( 4 ) 
fs1 ( 1 ) 
fs2 ( 1 ) 
fs3 ( 1 ) 
getArgs(){return ( 2 ) 
getCurrentKey(){ ( 1 ) 
getCurrentValue(){ ( 1 ) 
getProgress(){ ( 1 ) 
hdfs ( 2 ) 
hdfs.delete(OutPath, ( 1 ) 
hdfs.delete(OutPath1, ( 1 ) 
hdfs.delete(OutPath2, ( 1 ) 
hdfs.delete(OutPath3, ( 1 ) 
if(!flag){//循环第一次 ( 1 ) 
if(!process){ ( 2 ) 
if(Double.parseDouble(value.toString().substring(index+1)) ( 1 ) 
if(Model.wordsProbably.containsKey(tempkey)){ ( 1 ) 
if(hdfs.exists(OutPath)) ( 1 ) 
if(hdfs.exists(OutPath1)) ( 1 ) 
if(hdfs.exists(OutPath2)) ( 1 ) 
if(hdfs.exists(OutPath3)) ( 1 ) 
if(jobCtrl.allFinished()){ ( 1 ) 
if(otherArgs.length ( 1 ) 
if(tmp.matches("[a-zA-Z]+")) ( 1 ) 
implements ( 2 ) 
in ( 4 ) 
in.read(contextByte, ( 2 ) 
index ( 4 ) 
index)); ( 1 ) 
index));//得到单词所在的类 ( 1 ) 
index); ( 2 ) 
initialize(InputSplit ( 1 ) 
inputSplit, ( 2 ) 
int ( 8 ) 
isSplitable(JobContext ( 1 ) 
itr ( 2 ) 
itr.nextToken(); ( 1 ) 
itr.nextToken();//构建临时键值对<class:word>,在wordsProbably表中查找对应的概率 ( 1 ) 
jar命令指定执行文件并传入输入输出目录。 ( 1 ) 
job ( 1 ) 
job) ( 2 ) 
job.setInputFormatClass(MyFileInputFormat.class); ( 1 ) 
job.setJarByClass(DataPreparation.class); ( 1 ) 
job.setMapOutputKeyClass(Text.class); ( 1 ) 
job.setMapOutputValueClass(Text.class); ( 1 ) 
job.setMapperClass(MapsAndReduces.ClassifyMap.class); ( 1 ) 
job.setOutputFormatClass(SequenceFileOutputFormat.class); ( 1 ) 
job.setOutputKeyClass(Text.class); ( 1 ) 
job.setOutputValueClass(Text.class); ( 1 ) 
job.setReducerClass(MapsAndReduces.ClassifyReduce.class); ( 1 ) 
job.waitForCompletion(true) ( 1 ) 
job1 ( 1 ) 
job1.setCombinerClass(IntSumReducer.class); ( 1 ) 
job1.setInputFormatClass(MyFileInputFormat.class); ( 1 ) 
job1.setJarByClass(DataPreparation.class); ( 1 ) 
job1.setMapOutputKeyClass(Text.class);//map阶段的输出的key ( 1 ) 
job1.setMapOutputValueClass(IntWritable.class);//map阶段的输出的value ( 1 ) 
job1.setMapperClass(MapsAndReduces.MultiFileWordCountMap.class); ( 1 ) 
job1.setOutputFormatClass(SequenceFileOutputFormat.class); ( 1 ) 
job1.setOutputKeyClass(Text.class); ( 1 ) 
job1.setOutputKeyClass(Text.class);//reduce阶段的输出的key ( 1 ) 
job1.setOutputValueClass(IntWritable.class); ( 1 ) 
job1.setOutputValueClass(IntWritable.class);//reduce阶段的输出的value ( 1 ) 
job1.setReducerClass(IntSumReducer.class); ( 1 ) 
job2 ( 1 ) 
job2.setInputFormatClass(SequenceFileInputFormat.class); ( 1 ) 
job2.setJarByClass(DataPreparation.class); ( 1 ) 
job2.setMapOutputKeyClass(Text.class); ( 1 ) 
job2.setMapOutputValueClass(IntWritable.class); ( 1 ) 
job2.setMapperClass(MapsAndReduces.ClassWordCountMap.class); ( 1 ) 
job2.setOutputFormatClass(SequenceFileOutputFormat.class); ( 1 ) 
job2.setOutputKeyClass(Text.class); ( 1 ) 
job2.setOutputValueClass(IntWritable.class); ( 1 ) 
job2.setReducerClass(IntSumReducer.class); ( 1 ) 
job2.setReducerClass(MapsAndReduces.ClassWordCountReduce.class); ( 1 ) 
job3 ( 1 ) 
job3.setCombinerClass(MapsAndReduces.ExistingWordReduce.class); ( 1 ) 
job3.setInputFormatClass(SequenceFileInputFormat.class); ( 1 ) 
job3.setJarByClass(DataPreparation.class); ( 1 ) 
job3.setMapOutputKeyClass(Text.class); ( 1 ) 
job3.setMapOutputValueClass(IntWritable.class); ( 1 ) 
job3.setMapperClass(MapsAndReduces.ExistingWordMap.class); ( 1 ) 
job3.setOutputFormatClass(SequenceFileOutputFormat.class); ( 1 ) 
job3.setOutputKeyClass(Text.class); ( 1 ) 
job3.setOutputValueClass(IntWritable.class); ( 1 ) 
job3.setReducerClass(MapsAndReduces.ExistingWordReduce.class); ( 1 ) 
jobCtrl ( 1 ) 
jobCtrl.addJob(ctrljob1); ( 1 ) 
jobCtrl.addJob(ctrljob2); ( 1 ) 
jobCtrl.addJob(ctrljob3); ( 1 ) 
jobCtrl.stop(); ( 1 ) 
key ( 2 ) 
key, ( 6 ) 
key.set(fileSplit.getPath().toString()); ( 2 ) 
key.toString().indexOf(":"); ( 2 ) 
key1 ( 1 ) 
key2 ( 1 ) 
key3 ( 1 ) 
key3.toString().indexOf(":"); ( 1 ) 
key; ( 1 ) 
line ( 1 ) 
listStatus(JobContext ( 2 ) 
long ( 1 ) 
main(String[] ( 2 ) 
map(Text ( 4 ) 
map阶段，每个map负责一个文本样本在所有类中的概率计算。设此样本属于某个类的概率设为classProb，首先将该类的先验概率取log，作为classProb的初始值，然后对文本中所有单词在该类下的条件概率取Log，然后相加求和。得到此样本属于该类的概率，最后将key设为“文档名”，value设为“类名 ( 1 ) 
negatives ( 2 ) 
new ( 58 ) 
newKey ( 4 ) 
newKey.set(docID);//新的键值的key为<文档名> ( 1 ) 
newKey.set(key.toString().substring(0, ( 1 ) 
newKey.set(key.toString().substring(index+1)); ( 1 ) 
newKey.set(key3.toString().substring(0, ( 1 ) 
newValue ( 2 ) 
newValue); ( 2 ) 
newValue);//一份文档遍历在一个类中遍历完毕,则将结果写入文件,即<docID,<class:probably>> ( 1 ) 
newValue.set(className ( 1 ) 
newValue.set(tempClass); ( 1 ) 
nextKeyValue() ( 2 ) 
null; ( 7 ) 
one ( 2 ) 
one); ( 2 ) 
otherArgs ( 3 ) 
otherArgs; ( 2 ) 
otherArgs;} ( 2 ) 
otherArgs[0]); ( 1 ) 
otherArgs[1]+"/part-r-00000"; ( 1 ) 
otherArgs[2]+"/part-r-00000"; ( 2 ) 
otherArgs[3]+"/part-r-00000"; ( 1 ) 
path ( 1 ) 
path, ( 2 ) 
path1 ( 1 ) 
path1, ( 1 ) 
path2 ( 1 ) 
path2, ( 1 ) 
path3 ( 1 ) 
path3, ( 1 ) 
position ( 1 ) 
positives ( 2 ) 
printUsage() ( 1 ) 
printUsage(); ( 1 ) 
private ( 16 ) 
process ( 3 ) 
process? ( 1 ) 
protected ( 3 ) 
public ( 32 ) 
reader ( 5 ) 
reader.getPosition();//设置标记点，标记文档起始位置，方便后面再回来遍历 ( 1 ) 
reader.initialize(inputSplit, ( 1 ) 
reader.seek(position);//重置到前面定位的标记点 ( 1 ) 
reader1 ( 2 ) 
reader2 ( 2 ) 
reader3 ( 2 ) 
reader; ( 1 ) 
reduce(Text ( 2 ) 
res ( 2 ) 
res.addAll(Arrays.asList(files)); ( 2 ) 
res; ( 2 ) 
ret ( 2 ) 
return ( 16 ) 
run(String[] ( 2 ) 
setup(Context ( 1 ) 
static ( 18 ) 
super.listStatus(job); ( 2 ) 
table） ( 1 ) 
taskAttemptContext) ( 1 ) 
taskAttemptContext); ( 1 ) 
taskAttemptContext){ ( 1 ) 
taskAttemptContext.getConfiguration(); ( 1 ) 
tempClass ( 3 ) 
tempProbably ( 3 ) 
tempProbably){ ( 1 ) 
tempkey ( 1 ) 
tempvalue ( 3 ) 
tempvalue);//新的键值的value为<类名:概率>,即<class:probably> ( 1 ) 
theController ( 1 ) 
theController.start(); ( 1 ) 
this.conf ( 1 ) 
this.fileSplit ( 1 ) 
throws ( 16 ) 
tmp ( 1 ) 
totalWords ( 2 ) 
true ( 2 ) 
true); ( 4 ) 
true; ( 5 ) 
try ( 2 ) 
try{ ( 4 ) 
value ( 2 ) 
value); ( 2 ) 
value, ( 4 ) 
value.get()/totalWords);//P(c)=类c下的单词总数/整个训练样本的单词总数 ( 1 ) 
value.get();//得到训练集总单词数 ( 1 ) 
value.set(context); ( 2 ) 
value.toString().indexOf(":"); ( 1 ) 
value.toString().substring(0, ( 2 ) 
value.toString(); ( 1 ) 
value1 ( 1 ) 
value1.get()); ( 1 ) 
value1.get()*1.0); ( 1 ) 
value2 ( 1 ) 
value2.get(); ( 1 ) 
value3 ( 1 ) 
value:values){ ( 1 ) 
value; ( 1 ) 
values, ( 1 ) 
values,Context ( 1 ) 
void ( 12 ) 
while ( 1 ) 
while(itr.hasMoreTokens()){//内层循环遍历一份测试文档中的所有单词 ( 1 ) 
while(reader.next(key,value)){ ( 2 ) 
while(reader1.next(key1,value1)){ ( 1 ) 
while(reader2.next(key2,value2)){ ( 1 ) 
while(reader3.next(key3,value3)){ ( 1 ) 
while(true){ ( 1 ) 
word ( 1 ) 
word.set(new ( 1 ) 
word2...> ( 2 ) 
wordsProbably ( 2 ) 
wordsProbably.put(entry.getKey(), ( 1 ) 
wordsProbably.put(key3.toString(), ( 1 ) 
wordsProbably; ( 1 ) 
{//过滤掉以存在非字母的词 ( 1 ) 
}else{//否则当概率更大时就更新tempClass和tempProbably ( 1 ) 
}finally ( 2 ) 
}finally{ ( 4 ) 
—— ( 2 ) 
…… ( 1 ) 
① ( 1 ) 
② ( 1 ) 
③ ( 1 ) 
④ ( 1 ) 
。 ( 3 ) 
一、朴素贝叶斯分类器 ( 2 ) 
一种常见的参数选择方式是且，得到如下概率估计公式： ( 1 ) 
三、分类器的评估 ( 2 ) 
与出现过的单词数，然后采用1.5.1节中所描述的多项式模型： ( 1 ) 
与多项式模型一样，伯努利模型适用于离散特征的情况，所不同的是，多重伯努利事件空间是一种布尔独立模型的事件空间，为每一个词组建立一个二值随机变量。最简单的方式就是使用最大似然估计来估计概率，即式： ( 1 ) 
之后MapReduce过程则对样本进行分类，得到key为文件名，value为类名的结果。 ( 1 ) 
二、贝叶斯程序设计 ( 2 ) 
五、总结 ( 2 ) 
任务划分 ( 1 ) 
伯努利模型 ( 2 ) 
但是该分类器存在处理速度较慢的问题，主要原因是小文件过多，即使设置默认不分片，分片数量也与文件数量相同，从而使map数量达到5168个，使得MapReduce过程文件读取开销过大。后期可以采用合并小文件为序列化文件后再处理。 ( 1 ) 
位数 ( 1 ) 
作用 ( 1 ) 
使用log将乘法化为加法 ( 2 ) 
使用三台服务器搭建一个完全分布式Hadoop作为测试集群，服务器具体配置信息如表4所示。 ( 1 ) 
先验概率 ( 1 ) 
先验概率与类条件概率的具体算法与所取的模型有关，细节将在1.5节介绍。 ( 1 ) 
六、 ( 2 ) 
关键字：MapReduce贝叶斯 ( 1 ) 
其中，为类下文件总数，为整个训练样本的文件总数。 ( 1 ) 
其中，是类c下单词总数，为整个训练样本的单词总数。 ( 2 ) 
其中，每个a为X的一个特征属性，而且特征属性之间相互独立 ( 1 ) 
其中，表示训练样本包含多少种单词。 ( 1 ) 
其中，表示训练集中类别中词组出现的次数。表示训练集类别中的总词数，表示训练样本包含多少种单词。 ( 1 ) 
其中：表示类别含有词组的样本数量。平滑之后，多重伯努利模型的平滑估计如下式所示： ( 1 ) 
其中：表示训练集中类别中词组出现的次数。表示训练集类别中的总词数。加入平滑估计的概率如式： ( 1 ) 
内存 ( 1 ) 
出现次数>、<单词 ( 1 ) 
出现次数>的keyvalue结果将以Sequence文件储存在HDFS中，然后将测试集上传至HDFS执行分类过程。下面将分别对这两个过程讲解。 ( 1 ) 
分 ( 1 ) 
分别得到key为“类名 ( 1 ) 
分类 ( 2 ) 
分类器预测的类别 ( 1 ) 
分类过程利用数据准备过程提供的数据，在执行MapReduce之前，先进行先验概率和条件概率的初始化工作。将并将其以键值对的形式存入两个Hashtable中，用以高效查询概率。 ( 1 ) 
分类阶段 ( 2 ) 
则 ( 1 ) 
功能：得到每个类的单词总数 ( 2 ) 
功能：得到训练集中出现过的单词 ( 2 ) 
功能：根据得到的贝叶斯网络，对文档进行分类 ( 2 ) 
功能：负责提取所有文本中的单词 ( 2 ) 
单词 ( 1 ) 
单词”value为“条件概率”与key为“类”value为“先验概率”的数据，然后将结果保存至包装类Prediction的两个HashTable成员变量中，使得map函数能高效查询概率。 ( 1 ) 
单词”，value为“出现次数”的数据。 ( 1 ) 
单词”，value为“出现次数”的数据，在map过程中将key设置为“单词”，value设为“1”，reduce过程key不变，Value设为固定的“1”即可。 ( 1 ) 
单词”，value为“出现次数”的数据，在map过程中将key设置为类名，value不变，reduce使用IntSumReducer即可。 ( 1 ) 
原理 ( 2 ) 
可以看出Reduce相对Map阶段较慢，存在速度不匹配的问题，可以考虑增加Reduce的数量。 ( 1 ) 
号 ( 1 ) 
同理，类的先验概率为： ( 2 ) 
名 ( 1 ) 
四、测试 ( 2 ) 
因为分母相当于在数据库中X存在的概率，所以对于任何一个待分类项来说 ( 1 ) 
因为各特征值是独立的所以有： ( 1 ) 
图1 ( 1 ) 
图2 ( 1 ) 
图3 ( 1 ) 
图4 ( 1 ) 
图5 ( 1 ) 
图6 ( 1 ) 
图7 ( 1 ) 
图8 ( 1 ) 
在初始化过程中，依据第二个与第三个MapReduce程序的结果，先统计出训练集所包含的单词总数 ( 1 ) 
在文本分类中，特征为离散，不应使用高斯模型。 ( 1 ) 
在本次课程设计中，使用Hadoop实现了一个朴素贝叶斯分类器，利用MapReduce框架，实现了对大量数据的并行处理。最后通过测试文档的真实类别，计算分类模型的Precision，Recall和F1值 ( 1 ) 
多重伯努利模型仅仅考虑词组是否出现，而没有考虑出现的多少，而词频也是一个重要分类信息，所以在文本分类中，效果不如多项式模型。 ( 1 ) 
多项式模型 ( 2 ) 
天 ( 1 ) 
如图8所示，通过评估，最终测得宏平均与微平均的F1值均约为84%。 ( 1 ) 
如果贝叶斯是对多个类进行分类，则每个都会有Precision、Recall与F1，如何将多个性能度量组合成一个量？ ( 1 ) 
姓 ( 1 ) 
存放各个MapReduce程序的Map类与Reduce类 ( 1 ) 
学 ( 1 ) 
宏平均为每个类提供了相同的权重，而微平均为每个文档分类决策提供了相同的权重。 ( 1 ) 
宏平均（Macro-Averaging）：是对数据集中的每一个实例不分类别进行统计建立全局混淆矩阵，然后计算相应指标。 ( 1 ) 
实际应用中，多项式模型已经表明优于多重伯努利模型。所以我将多项式模型作为实现模型。 ( 1 ) 
实际的类别 ( 1 ) 
将程序分解为两个过程，分别为数据准备与贝叶斯分类，共四个MapReduce任务，分别为MultiFileWordCount、ClassWordCount、ExistingWord、Classify。 ( 1 ) 
巡 ( 1 ) 
工程中包含的文件 ( 1 ) 
希 ( 1 ) 
当=1时，称作Laplace平滑，当0<<1时，称作Lidstone平滑，=0时即不做平滑。 ( 1 ) 
当特征是连续变量的时候，运用多项式模型就会导致很多（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，应该采用高斯模型。 ( 1 ) 
得到分类结果后，我们使用混淆矩阵的方式进行评测，混淆矩阵如表2所示。 ( 1 ) 
得到每个类的单词总数 ( 1 ) 
微平均与宏平均 ( 2 ) 
微平均（Micro ( 1 ) 
总 ( 1 ) 
所以在计算概率时，会对数据进行平滑，具体公式为： ( 1 ) 
执行完数据准备过程后，<类名 ( 1 ) 
找到一个已知分类的待分类项集合，这个集合叫做训练样本集。 ( 1 ) 
提取文件中的类与单词，并进行词频统计 ( 1 ) 
摘要 ( 1 ) 
数据准备 ( 1 ) 
数据准备阶段 ( 2 ) 
数据准备阶段的主程序 ( 1 ) 
数据选择、训练集设置 ( 2 ) 
数据集的设置 ( 1 ) 
数据集选取的Country文件夹，即以国家为分类目标，并且挑选了一些文档数较多的国家。测试集则以下列规则设定。 ( 1 ) 
数： ( 1 ) 
整个项目中包含的文件如表5所示。 ( 1 ) 
文件名 ( 1 ) 
文本分类过程中，涉及到多个属性的概率值相乘的情况，当足够多的小数相乘时就可能会因为数据下溢而导致结果出错。这时可以借助对数运算的性质，即，使得求积运算转化为求和运算，而log函数是单调递增函数，所以可以直接根据log运算的结果判断文档在各个类中的概率的大小而不影响最终分类结果。 ( 1 ) 
文档总数 ( 1 ) 
日 ( 1 ) 
是类别为的样本个数，n是特征的维数，是类别为的样本中，第i维特征的值是的样本个数，是平滑值。 ( 1 ) 
最后的训练集及测试集设置如表3所示。 ( 1 ) 
月 ( 1 ) 
有一个类别集合为： ( 1 ) 
服务器配置信息 ( 1 ) 
本课程设计用MapReduce算法实现贝叶斯分类器的训练过程，能对测试集文档进行分类测试。并且重写了InputFormat与RecordReader，实现了对特定目录结构的读取，设计包含三个步骤：数据准备及预处理阶段，样本分类阶段，评估阶段。最后利用测试文档的真实类别，计算分类模型的Precision，Recall和F1值。 ( 1 ) 
朴素贝叶斯分类器仍然是文本分类的一种热门方法，文本分类是以词频为特征判断文件所属类别或其他（如垃圾邮件、合法性、区域分类等等）的问题。贝叶斯分类器只需花费线性时间，而不需要其他很多类型的分类器所使用的费时的迭代逼近。通过适当的预处理，它可以与这个领域更先进的方法（包括支持向量机）相竞争。 ( 1 ) 
朴素贝叶斯分类器是一系列以假设特征之间强独立条件下运用贝叶斯定理为基础的简单概率分类器。 ( 1 ) 
朴素贝叶斯方法有个缺点是对数据稀疏问题过于敏感。若某个单词在训练集中不存在而在某个测试文档中出现，则会出现后验概率为0的情况。 ( 1 ) 
朴素贝叶斯有三种常见的模型：多项式模型、伯努利模型、高斯模型。文本分类一般使用多项式模型或伯努利模型，高斯模型一般应用在特征为连续的情况（如身高）。 ( 1 ) 
朴素贝叶斯的三种常见模型 ( 2 ) 
机器 ( 1 ) 
条件概率 ( 1 ) 
样本分类阶段的主程序 ( 1 ) 
根据公式： ( 1 ) 
根据图中的数据我们可以看到，在第一个job运行时，共有5168个map，即与Slip个数一致（Record中将是否分片设置为false，所以一个文件就是一个分片，而测试文件共有5168个）。 ( 1 ) 
根据数据准备阶段的结果，计算概率，存入Hashtable ( 1 ) 
概率>”的数据。 ( 1 ) 
概率>、<类名2 ( 1 ) 
概率>、…<类名n ( 1 ) 
概率”写入上下文。 ( 1 ) 
模型评估方法 ( 2 ) 
武 ( 1 ) 
求后验概率。在朴素贝叶斯算法中，待分类项的每个特征属性都是条件独立的，由贝叶斯公式 ( 1 ) 
测试 ( 2 ) 
测试文档数量 ( 1 ) 
混淆表 ( 1 ) 
混淆表（confusion ( 1 ) 
然后我们用Precision、Recall、F1来描述分类器的效果。 ( 1 ) 
然后，我们需要重写RecordReader，来定义送至Map任务的KeyValue数据格式。因为在MyInputFormat中设置了默认不分片，所以RecordReader收到的InputSplit为一个完整的文件。之后，我们将Key设置为该文件的路径，Value设置为文件的内容。 ( 1 ) 
环境配置 ( 2 ) 
由于工程源码过多，完全打印需要大约25页，所以只选取部分源码附后，并且省略程序的包导入部分。 ( 1 ) 
由于输入文件都为小于1MB的小文件，所以在InputFormat中设置为默认不分片，即重写isSplitable()，将返回值设为false。 ( 1 ) 
目录 ( 1 ) 
目：基于Hadoop的朴素贝叶斯分类器 ( 1 ) 
真实类别是yes的样本中有多少被分为yes： ( 1 ) 
研究生课程论文（报告） ( 1 ) 
第一个MapReduce程序的功能是对特定目录结构进行读取，然后进行类的单词统计，得到key为“类名 ( 1 ) 
第一个job执行时的监控界面 ( 1 ) 
第三个MapReduce的任务为统计训练集中出现过的单词。得到key为“单词”，此时的value无意义，设置为“1”（本来想把value设置为NullWriterable，但是会出现Shuffle阶段只排序并没有完成去重的情况）。 ( 1 ) 
第二个MapReduce的任务为统计每个类所包含的单词总数。得到key为“类名”，value为“单词数”的数据。 ( 1 ) 
简介 ( 2 ) 
简化流程如下： ( 1 ) 
类别 ( 1 ) 
系统版本 ( 1 ) 
经过Shuffle阶段后，Reduce端收到key为“文档名”，value为“<类名1 ( 1 ) 
统计得到在各类别下各个特征属性的条件概率估计。即： ( 1 ) 
统计训练集中出现过的单词 ( 1 ) 
而reduce个数则是由程序指定的（或者作为命令行参数传入），运行时未指定，所以默认为1。 ( 1 ) 
自定义的InputFormat类 ( 1 ) 
自定义的InputFormat继承自FileInputFormat，FileInputFormat中的listStatus()函数会使用默认路径过滤器过滤给定路径列表中的文件/目录，最后返回一个List<FileStatus>，这个List中的文件会被后续处理。FileInputFormat中的listStatus()函数默认只会返回当前目录下的所有文件，而无法处理子目录下的文件。所以我们在知道目录结构的情况下，将listStatus()重写，返回二级目录下的所有文件列表。 ( 1 ) 
自定义的RecordReader ( 1 ) 
若 ( 1 ) 
若有一个待分类项： ( 1 ) 
行数 ( 1 ) 
表1 ( 1 ) 
表2 ( 1 ) 
表3 ( 1 ) 
表4 ( 1 ) 
表5 ( 1 ) 
被分类为yes的样本中有多少真实类别是yes： ( 1 ) 
要求出第四项中的后验概率，就需要分别求出在第三项中的各个条件概率，其步骤是: ( 1 ) 
计算 ( 1 ) 
计算机学院 ( 1 ) 
计算条件概率 ( 1 ) 
计算条件概率与先验概率，并对样本进行贝叶斯分类 ( 1 ) 
训练文档数量 ( 1 ) 
评估结果 ( 1 ) 
评估阶段的主程序 ( 1 ) 
评卷人： ( 1 ) 
该模型常用于文本分类，特征是单词，值是单词的出现次数。多项式模型的最大似然估计计算如式： ( 1 ) 
说明 ( 1 ) 
课程指导教师 ( 1 ) 
输入文件为第一步所得到的key为“类名 ( 2 ) 
输入目录结构 ( 1 ) 
输入：args[0],即MyFileInputFormat的输出分片,key为<文件路径>,value为文件内容<word1 ( 1 ) 
输入：args[0],经过MyFileInputFormat的处理后,key为<文件路径>,value为文件内容<word1 ( 1 ) 
输入：args[1],输入格式为<<class,word>,counts> ( 2 ) 
输入：args[1],输入格式为<<class:word>,counts> ( 2 ) 
输入：args[4],测试文件的路径,经过InputFormat处理后的数据格式为<Path, ( 2 ) 
输出：args[1],key为<类名:单词>,value为单词出现次数,即<<Class:word>,TotalCounts> ( 2 ) 
输出：args[2],输出key为类名,value为单词总数.格式为<class,WordCount> ( 2 ) 
输出：args[3],输出key为出现过的单词,value为1.格式为<word,one> ( 2 ) 
输出：args[5],输出每一份文档经贝叶斯分类后所对应的类,格式为<doc,class> ( 2 ) 
辜 ( 1 ) 
过程 ( 1 ) 
这里是依赖于词组的参数。对所有词组*取是一种常见选择（即Laplace平滑）。得到最终的概率估计公式为： ( 1 ) 
通过此次课程设计作业，我学会了Hadoop的环境搭建和基本的设计架构，掌握了大数据处理的基本方法，对HDFS、MapReduce等内部实现细节有一个基本的认识。 ( 1 ) 
都是常数固定的。再求后验概率的时候只用考虑分子即可。 ( 1 ) 
附录 ( 2 ) 
附： ( 1 ) 
院（系、所） ( 1 ) 
题 ( 1 ) 
饶 ( 1 ) 
首先，由于输入目录“Country”为两级目录，二级目录的文件夹名即为分类的类名，若我们需要将文件全部读入，并且得到类名信息，我们需要自定义InputFormat与RecordReader。 ( 1 ) 
高斯模型 ( 2 ) 
（1） ( 1 ) 
（1）首先使用hadoop ( 1 ) 
（2个，12核24线程） ( 1 ) 
（2个，16核32线程） ( 2 ) 
（2） ( 1 ) 
（2）将程序打包为jar文件，使用hadoop ( 1 ) 
（3） ( 1 ) 
（DataPrepare类） ( 1 ) 
（Prediction类） ( 1 ) 

